# generators/scraper_generator.py
from pathlib import Path
from typing import Any, Dict

from .base_generator import BaseGenerator


class ScraperGenerator(BaseGenerator):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä»•æ§˜ç”Ÿæˆã‚¯ãƒ©ã‚¹"""

    def generate(self) -> Dict[str, Any]:
        """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä»•æ§˜ç”Ÿæˆ"""
        scraper_path = self.base_path / "rea-scraper"

        if not scraper_path.exists():
            self.print_status("âš ï¸ rea-scraperãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
            return {}

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒ£ãƒ³
        scrapers_dir = scraper_path / "src" / "scrapers"
        scraper_files = []

        if scrapers_dir.exists():
            scraper_files = list(scrapers_dir.rglob("*.py"))
            scraper_files = [f for f in scraper_files if f.name != "__init__.py"]

        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼æ¦‚è¦ç”Ÿæˆ
        content = f"""# ğŸ•·ï¸ REA ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä»•æ§˜

## ğŸ“‹ æ¦‚è¦
- **ç”Ÿæˆæ—¥æ™‚**: {self.get_timestamp()}
- **å¯¾è±¡ã‚µã‚¤ãƒˆ**: ãƒ›ãƒ¼ãƒ ã‚ºï¼ˆå®Ÿè£…æ¸ˆã¿ï¼‰
- **ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ**: rea-scraper
- **æ¤œå‡ºãƒ•ã‚¡ã‚¤ãƒ«æ•°**: {len(scraper_files)}

## ğŸ¯ ä¸»è¦æ©Ÿèƒ½
- **URLåé›†**: ç‰©ä»¶ä¸€è¦§ãƒšãƒ¼ã‚¸ã‹ã‚‰ç‰©ä»¶URLã‚’åé›†
- **è©³ç´°æŠ½å‡º**: å„ç‰©ä»¶ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°æƒ…å ±ã‚’æŠ½å‡º
- **ç”»åƒãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰**: ç‰©ä»¶ç”»åƒã®è‡ªå‹•ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
- **ãƒ‡ãƒ¼ã‚¿ä¿å­˜**: PostgreSQLã«è‡ªå‹•ä¿å­˜

## ğŸ—ï¸ ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ§‹é€ 
```
rea-scraper/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # ãƒ¡ã‚¤ãƒ³ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆ
â”‚   â”œâ”€â”€ scrapers/            # ã‚µã‚¤ãƒˆåˆ¥ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
â”‚   â”‚   â”œâ”€â”€ base/            # åŸºåº•ã‚¯ãƒ©ã‚¹
â”‚   â”‚   â””â”€â”€ homes/           # ãƒ›ãƒ¼ãƒ ã‚ºå¯¾å¿œ
â”‚   â”œâ”€â”€ utils/               # ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£
â”‚   â”‚   â”œâ”€â”€ selenium_manager.py
â”‚   â”‚   â”œâ”€â”€ logger.py
â”‚   â”‚   â””â”€â”€ process_manager.py
â”‚   â””â”€â”€ config/              # è¨­å®šç®¡ç†
â”œâ”€â”€ data/                    # ãƒ‡ãƒ¼ã‚¿ä¿å­˜å…ˆ
â”œâ”€â”€ logs/                    # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«
â””â”€â”€ requirements.txt         # ä¾å­˜é–¢ä¿‚
```

## ğŸ”§ å®Ÿè¡Œæ–¹æ³•

### URLåé›†
```bash
#cd rea-scraper
source ../venv/bin/activate
python -m src.main collect-urls --max-pages 10
```

### ãƒãƒƒãƒå‡¦ç†
```bash
python -m src.main process-batch --batch-size 10 --show-sample
```

### å…¨è‡ªå‹•å‡¦ç†
```bash
python -m src.main process-all --batch-size 10 --interval 300 --save
```

### çµ±è¨ˆç¢ºèª
```bash
python -m src.main queue-stats
```

## ğŸ› ï¸ ä¸»è¦ã‚¯ãƒ©ã‚¹
- **HomesPropertyScraper**: ãƒ›ãƒ¼ãƒ ã‚ºå°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
- **SeleniumManager**: ãƒ–ãƒ©ã‚¦ã‚¶åˆ¶å¾¡ãƒ»Botå¯¾ç­–
- **URLQueue**: URLç®¡ç†ãƒ»æ°¸ç¶šåŒ–
- **DatabaseSaver**: ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜

## ğŸ“Š ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹
- **å‡¦ç†é€Ÿåº¦**: ç´„11ç§’/ç‰©ä»¶
- **æˆåŠŸç‡**: 100%ï¼ˆãƒ†ã‚¹ãƒˆå®Ÿè¡Œåˆ†ï¼‰
- **åé›†å®Ÿç¸¾**: 279URLï¼ˆ96ä»¶æœ‰åŠ¹ï¼‰

## ğŸ¤– DBæ¥ç¶šçµ±ä¸€å¯¾å¿œ
- **å¾“æ¥**: å€‹åˆ¥ã®DBæ¥ç¶šå‡¦ç†
- **æ–°æ–¹å¼**: `shared/database.py` ä½¿ç”¨æ¨å¥¨
- **æ¥ç¶šç¢ºèª**: `python shared/database.py`

## ğŸ¤– ä½¿ç”¨ä¾‹
```bash
# ç’°å¢ƒèµ·å‹•
#cd /Users/yaguchimakoto/my_programing/REA
source venv/bin/activate
#cd rea-scraper

# å¯¾è©±å‹å®Ÿè¡Œ
./scripts/start_scraping.sh

# ãƒãƒƒã‚¯ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ
nohup ./scripts/monitor_scraper.sh > logs/monitor.log 2>&1 &
```
"""

        # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
        scraper_dir = self.get_output_dir("03_scraper")
        self.save_content(content, scraper_dir / "README.md")

        self.print_status("âœ… ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ä»•æ§˜ç”Ÿæˆå®Œäº†")
        return {"scraper_path": str(scraper_path), "scraper_files": len(scraper_files)}
