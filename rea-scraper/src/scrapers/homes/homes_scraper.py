"""
ãƒ›ãƒ¼ãƒ ã‚ºä¸å‹•ç”£å£²è²·ç‰©ä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªæœ€å¤§æ´»ç”¨ç‰ˆ
"""
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional
from urllib.parse import urljoin

from bs4 import BeautifulSoup
from loguru import logger

# ãƒ‘ã‚¹è¨­å®šï¼ˆã‚ˆã‚Šç°¡æ½”ã«ï¼‰
project_root = Path(__file__).resolve()
while project_root.name != "REA" and project_root.parent != project_root:
    project_root = project_root.parent

if project_root.name != "REA":
    raise ValueError("REAãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

rea_scraper_root = project_root / "rea-scraper"
sys.path.insert(0, str(project_root))
sys.path.insert(1, str(rea_scraper_root))

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from src.scrapers.base.base_scraper import BaseScraper
from src.utils.decorators import retry

from shared.database import READatabase
from shared.formatters import clean_phone_number, extract_listing_id, normalize_address
from shared.real_estate_utils import (
    determine_property_type,
    parse_area,
    parse_construction_year,
    parse_sale_price,
)

# å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from shared.scrapers_common import (
    RateLimiter,
    URLQueue,
    create_property_base_data,
    extract_contractor_info,
)


class HomesPropertyScraper(BaseScraper):
    """ãƒ›ãƒ¼ãƒ ã‚ºå£²è²·ç‰©ä»¶å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ç‰ˆï¼‰"""

    def __init__(self):
        super().__init__(
            site_name="homes", base_url="https://www.homes.co.jp", login_required=False
        )

        self.url_queue = URLQueue(site_name="homes")
        self.rate_limiter = RateLimiter(min_delay=3.0, max_delay=8.0)

        # ãƒ›ãƒ¼ãƒ ã‚ºå›ºæœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿
        self.selectors = {
            "property_link": 'a[href*="/kodate/b-"], a[href*="/mansion/b-"], a[href*="/tochi/b-"]',
            "next_page": ['a:contains("æ¬¡ã¸")', "a.next", ".pagination__next a"],
            "detail_title": "h1.name, h1",
            "company_name": ".shopName",
            "company_phone": ".freeCall, span.u-text-2xl",
        }

        # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒãƒƒãƒ”ãƒ³ã‚°å®šç¾©
        self.table_mapping = {
            "æ‰€åœ¨åœ°": (
                "address",
                lambda v: normalize_address(re.sub(r"(åœ°å›³ã‚’è¦‹ã‚‹|ç„¡æ–™).*", "", v).strip()),
            ),
            "åœŸåœ°é¢ç©": ("land_area", parse_area),
            "å»ºç‰©é¢ç©": ("building_area", parse_area),
            "ç¯‰å¹´æœˆ": ("construction_year", parse_construction_year),
            "æ§‹é€ ": ("structure", lambda v: v),
        }

    def collect_property_urls(self, base_url: str, max_pages: int = 50) -> int:
        """Step 1: ç‰©ä»¶URLã‚’åé›†"""
        logger.info(f"ğŸ•·ï¸ [Homes] URLåé›†é–‹å§‹: {base_url}")
        collected_urls = []
        current_url = base_url

        for page_num in range(1, max_pages + 1):
            logger.info(f"ğŸ“„ Page {page_num}: {current_url}")

            html = self.get_page_source(current_url)
            if not html:
                break

            soup = BeautifulSoup(html, "html.parser")

            # ç‰©ä»¶ãƒªãƒ³ã‚¯åé›†
            links = soup.select(self.selectors["property_link"])
            page_urls = [
                urljoin(self.base_url, link["href"])
                for link in links
                if link.get("href")
            ]
            collected_urls.extend(page_urls)

            logger.info(f"âœ… Found {len(page_urls)} properties")

            # æ¬¡ãƒšãƒ¼ã‚¸
            next_link = self._find_next_page(soup, current_url)
            if not next_link:
                break

            current_url = next_link
            self.rate_limiter.wait()

        new_count = self.url_queue.add_urls(collected_urls)
        logger.info(f"ğŸ¯ åé›†å®Œäº†: {new_count}ä»¶ã®æ–°è¦URL")
        return new_count

    def process_batch(
        self, batch_size: int = 10, save_to_db: bool = False
    ) -> List[Dict[str, Any]]:
        """Step 2: ãƒãƒƒãƒå‡¦ç†"""
        urls = self.url_queue.get_next_batch(batch_size)
        if not urls:
            return []

        properties = []
        for i, url in enumerate(urls, 1):
            logger.info(f"ğŸ” Processing {i}/{len(urls)}: {url}")

            try:
                property_data = self.scrape_property_detail(url)
                if property_data:
                    properties.append(property_data)
                    logger.success(f"âœ“ {property_data.get('title', 'Unknown')}")

                    # DBä¿å­˜
                    if save_to_db:
                        self.save_to_database(property_data)

                self.url_queue.mark_completed(url)
            except Exception as e:
                logger.error(f"âœ— Failed: {e}")
                self.url_queue.mark_completed(url)

            if i < len(urls):
                self.rate_limiter.wait()

        return properties

    @retry(max_attempts=3)
    def scrape_property_detail(self, url: str) -> Optional[Dict[str, Any]]:
        """ç‰©ä»¶è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        html = self.get_page_source(url)
        if not html:
            return None

        soup = BeautifulSoup(html, "html.parser")

        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        property_data = create_property_base_data(url, "homes")

        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = soup.select_one(self.selectors["detail_title"])
        if title_elem:
            property_data["title"] = title_elem.get_text(strip=True)

        # ä¾¡æ ¼
        price_data = self._extract_price(soup)
        property_data.update(price_data)

        # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆæ±ç”¨çš„ã«ï¼‰
        table_data = self._extract_table_data_generic(soup)
        property_data.update(table_data)

        # å…ƒè«‹ä¼šç¤¾æƒ…å ±
        contractor_info = self._extract_contractor_info(soup)
        property_data.update(contractor_info)

        # ç‰©ä»¶ã‚¿ã‚¤ãƒ—ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        property_data["property_type"] = determine_property_type(
            url, property_data.get("title", "")
        )

        # listing_id
        property_data["listing_id"] = extract_listing_id(url, "homes")

        return property_data

    def _extract_price(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ä¾¡æ ¼æŠ½å‡ºï¼ˆæ±ç”¨åŒ–ï¼‰"""
        for elem in soup.find_all(["td", "span", "div"]):
            text = elem.get_text(strip=True)
            if "ä¸‡å††" in text and text[0].isdigit():
                match = re.search(r"(\d{1,5}(?:,\d{3})*(?:\.\d+)?)\s*ä¸‡å††", text)
                if match:
                    return {
                        "price_raw": match.group(0),
                        "price": parse_sale_price(match.group(0)),
                    }
        return {}

    def _extract_table_data_generic(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆæ±ç”¨åŒ–ï¼‰"""
        result = {}

        for tr in soup.find_all("tr"):
            th = tr.find("th")
            td = tr.find("td")
            if th and td:
                header = th.get_text(strip=True)
                value = td.get_text(strip=True)

                # ãƒãƒƒãƒ”ãƒ³ã‚°ã«åŸºã¥ã„ã¦å‡¦ç†
                for key_pattern, (field_name, processor) in self.table_mapping.items():
                    if key_pattern in header:
                        result[field_name] = processor(value)
                        break

        return result

    def _extract_contractor_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """å…ƒè«‹ä¼šç¤¾æƒ…å ±æŠ½å‡º"""
        contractor_info = {}

        # ä¼šç¤¾å
        company_elem = soup.select_one(self.selectors["company_name"])
        if company_elem:
            contractor_info["contractor_company_name"] = company_elem.get_text(
                strip=True
            )

        # é›»è©±ç•ªå·
        phone_elem = soup.select_one(self.selectors["company_phone"])
        if phone_elem:
            phone_text = phone_elem.get_text(strip=True)
            contractor_info["contractor_phone"] = clean_phone_number(phone_text)

        return contractor_info

    def _find_next_page(self, soup: BeautifulSoup, current_url: str) -> Optional[str]:
        """æ¬¡ãƒšãƒ¼ã‚¸æ¤œç´¢"""
        for selector in self.selectors["next_page"]:
            next_elem = soup.select_one(selector)
            if next_elem and next_elem.get("href"):
                return urljoin(current_url, next_elem["href"])
        return None

    def save_to_database(self, property_data: Dict[str, Any]):
        """ç‰©ä»¶ãƒ‡ãƒ¼ã‚¿ã‚’DBã«ä¿å­˜"""
        try:
            query = """
            INSERT INTO properties (
                homes_record_id,
                building_property_name,
                price,
                address_name,
                contractor_company_name,
                contractor_phone,
                created_at
            ) VALUES (%s, %s, %s, %s, %s, %s, NOW())
            ON CONFLICT (homes_record_id) DO UPDATE SET
                building_property_name = EXCLUDED.building_property_name,
                price = EXCLUDED.price,
                updated_at = NOW()
            """

            params = (
                property_data.get("listing_id"),
                property_data.get("title"),
                property_data.get("price"),
                property_data.get("address"),
                property_data.get("contractor_company_name"),
                property_data.get("contractor_phone"),
            )

            READatabase.execute_query(query, params)
            logger.success(f"âœ… DBä¿å­˜å®Œäº†: {property_data.get('title')}")

        except Exception as e:
            logger.error(f"âŒ DBä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_queue_stats(self) -> Dict[str, int]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.url_queue.get_stats()

    def reset_queue(self):
        """ã‚­ãƒ¥ãƒ¼ãƒªã‚»ãƒƒãƒˆ"""
        self.url_queue.reset()

    # äº’æ›æ€§ãƒ¡ã‚½ãƒƒãƒ‰
    def scrape_listing_page(self, url: str) -> List[Dict[str, Any]]:
        self.collect_property_urls(url, max_pages=1)
        return []

    def scrape_detail_page(self, url: str) -> Optional[Dict[str, Any]]:
        return self.scrape_property_detail(url)


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    logger.info("ğŸš€ [Homes] Test execution start")

    scraper = HomesPropertyScraper()

    try:
        # URLåé›†ãƒ†ã‚¹ãƒˆ
        test_url = "https://www.homes.co.jp/kodate/hokkaido/list/"
        collected = scraper.collect_property_urls(test_url, max_pages=2)
        logger.info(f"âœ… Collected {collected} URLs")

        # ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆï¼ˆDBä¿å­˜ä»˜ãï¼‰
        properties = scraper.process_batch(batch_size=3, save_to_db=True)

        if properties:
            logger.info(f"âœ… Scraped {len(properties)} properties")
            for prop in properties[:2]:  # æœ€åˆã®2ä»¶ã ã‘è¡¨ç¤º
                logger.info(f"  - {prop.get('title')}")
                logger.info(f"    ğŸ’° {prop.get('price_raw')}")
                logger.info(f"    ğŸ¢ {prop.get('contractor_company_name', 'N/A')}")

        # çµ±è¨ˆ
        stats = scraper.get_queue_stats()
        logger.info(f"ğŸ“Š Stats: {stats}")

    except Exception as e:
        logger.error(f"âŒ Test failed: {e}")
    finally:
        scraper.close()
        logger.info("ğŸ Test completed")
