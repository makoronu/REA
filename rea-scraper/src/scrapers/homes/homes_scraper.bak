"""
ãƒ›ãƒ¼ãƒ ã‚ºä¸å‹•ç”£å£²è²·ç‰©ä»¶ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼
å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ç‰ˆ
"""
import sys
from pathlib import Path
from typing import Dict, List, Optional, Any
from datetime import datetime
import re
from urllib.parse import urljoin

from bs4 import BeautifulSoup
from loguru import logger

# ãƒ‘ã‚¹è¨­å®šï¼ˆä¿®æ­£ç‰ˆï¼‰
import os
# homes_scraper.py â†’ homes â†’ scrapers â†’ src â†’ rea-scraper â†’ REA
project_root = Path(__file__).parent.parent.parent.parent.parent  # 5éšå±¤ä¸ŠãŒREAãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ
rea_scraper_root = project_root / "rea-scraper"
sys.path.insert(0, str(project_root))
sys.path.insert(1, str(rea_scraper_root))

print(f"ğŸ” Project root: {project_root}")
print(f"ğŸ” Rea-scraper root: {rea_scraper_root}")
print(f"ğŸ” Shared exists: {(project_root / 'shared').exists()}")

# shared ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç¢ºèª
shared_dir = project_root / 'shared'
if shared_dir.exists():
    print(f"âœ… Shared files found: {len(list(shared_dir.glob('*.py')))} files")
else:
    print(f"ğŸš¨ Shared directory not found at: {shared_dir}")
    sys.exit(1)

# å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚¤ãƒ³ãƒãƒ¼ãƒˆ
from shared.scrapers_common import URLQueue, RateLimiter, extract_contractor_info, create_property_base_data
from shared.real_estate_utils import parse_sale_price, parse_area, parse_construction_year, determine_property_type
from shared.formatters import normalize_address, extract_listing_id, clean_phone_number
from shared.database import READatabase

# æ—¢å­˜ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«
from src.scrapers.base.base_scraper import BaseScraper
from src.utils.decorators import retry


class HomesPropertyScraper(BaseScraper):
    """ãƒ›ãƒ¼ãƒ ã‚ºå£²è²·ç‰©ä»¶å°‚ç”¨ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ‘ãƒ¼ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ç‰ˆï¼‰"""
    
    def __init__(self):
        super().__init__(
            site_name="homes",
            base_url="https://www.homes.co.jp",
            login_required=False
        )
        
        self.url_queue = URLQueue(site_name="homes")
        self.rate_limiter = RateLimiter(min_delay=3.0, max_delay=8.0)
        
        # ãƒ›ãƒ¼ãƒ ã‚ºå›ºæœ‰ã®ã‚»ãƒ¬ã‚¯ã‚¿
        self.selectors = {
            'property_link': 'a[href*="/kodate/b-"], a[href*="/mansion/b-"], a[href*="/tochi/b-"]',
            'next_page': ['a:contains("æ¬¡ã¸")', 'a.next', '.pagination__next a'],
            'detail_title': 'h1.name, h1',
            'company_name': '.shopName',
            'company_phone': '.freeCall, span.u-text-2xl',
        }
    
    def collect_property_urls(self, base_url: str, max_pages: int = 50) -> int:
        """Step 1: ç‰©ä»¶URLã‚’åé›†"""
        logger.info(f"ğŸ•·ï¸ [Homes] URLåé›†é–‹å§‹: {base_url}")
        collected_urls = []
        current_url = base_url
        
        for page_num in range(1, max_pages + 1):
            logger.info(f"ğŸ“„ Page {page_num}: {current_url}")
            
            html = self.get_page_source(current_url)
            if not html:
                break
                
            soup = BeautifulSoup(html, 'html.parser')
            
            # ç‰©ä»¶ãƒªãƒ³ã‚¯åé›†
            links = soup.select(self.selectors['property_link'])
            page_urls = [urljoin(self.base_url, link['href']) for link in links if link.get('href')]
            collected_urls.extend(page_urls)
            
            logger.info(f"âœ… Found {len(page_urls)} properties")
            
            # æ¬¡ãƒšãƒ¼ã‚¸
            next_link = self._find_next_page(soup, current_url)
            if not next_link:
                break
                
            current_url = next_link
            self.rate_limiter.wait()
        
        new_count = self.url_queue.add_urls(collected_urls)
        logger.info(f"ğŸ¯ åé›†å®Œäº†: {new_count}ä»¶ã®æ–°è¦URL")
        return new_count
    
    def process_batch(self, batch_size: int = 10) -> List[Dict[str, Any]]:
        """Step 2: ãƒãƒƒãƒå‡¦ç†"""
        urls = self.url_queue.get_next_batch(batch_size)
        if not urls:
            return []
        
        properties = []
        for i, url in enumerate(urls, 1):
            logger.info(f"ğŸ” Processing {i}/{len(urls)}: {url}")
            
            try:
                property_data = self.scrape_property_detail(url)
                if property_data:
                    properties.append(property_data)
                    logger.success(f"âœ“ {property_data.get('title', 'Unknown')}")
                self.url_queue.mark_completed(url)
            except Exception as e:
                logger.error(f"âœ— Failed: {e}")
                self.url_queue.mark_completed(url)
            
            if i < len(urls):
                self.rate_limiter.wait()
        
        return properties
    
    @retry(max_attempts=3)
    def scrape_property_detail(self, url: str) -> Optional[Dict[str, Any]]:
        """ç‰©ä»¶è©³ç´°ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°"""
        html = self.get_page_source(url)
        if not html:
            return None
        
        soup = BeautifulSoup(html, 'html.parser')
        
        # åŸºæœ¬ãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        property_data = create_property_base_data(url, "homes")
        
        # ã‚¿ã‚¤ãƒˆãƒ«
        title_elem = soup.select_one(self.selectors['detail_title'])
        if title_elem:
            property_data['title'] = title_elem.get_text(strip=True)
        
        # ä¾¡æ ¼ï¼ˆãƒ›ãƒ¼ãƒ ã‚ºå›ºæœ‰å‡¦ç†ï¼‰
        self._extract_price(soup, property_data)
        
        # ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿
        self._extract_table_data(soup, property_data)
        
        # å…ƒè«‹ä¼šç¤¾æƒ…å ±ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        contractor_info = self._extract_contractor_info(soup)
        property_data.update(contractor_info)
        
        # ç‰©ä»¶ã‚¿ã‚¤ãƒ—ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        property_data['property_type'] = determine_property_type(url, property_data.get('title', ''))
        
        # å¾Œå‡¦ç†ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªä½¿ç”¨ï¼‰
        self._post_process(property_data)
        
        return property_data
    
    def _extract_price(self, soup: BeautifulSoup, property_data: Dict[str, Any]):
        """ä¾¡æ ¼æŠ½å‡ºï¼ˆãƒ›ãƒ¼ãƒ ã‚ºå›ºæœ‰ï¼‰"""
        for td in soup.find_all('td'):
            text = td.get_text(strip=True)
            if 'ä¸‡å††' in text and text[0].isdigit():
                match = re.search(r'(\d{1,5}(?:,\d{3})*(?:\.\d+)?)\s*ä¸‡å††', text)
                if match:
                    property_data['price_raw'] = match.group(0)
                    property_data['price'] = parse_sale_price(match.group(0))  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
                    break
    
    def _extract_table_data(self, soup: BeautifulSoup, property_data: Dict[str, Any]):
        """ãƒ†ãƒ¼ãƒ–ãƒ«ãƒ‡ãƒ¼ã‚¿æŠ½å‡º"""
        for tr in soup.find_all('tr'):
            th = tr.find('th')
            td = tr.find('td')
            if th and td:
                header = th.get_text(strip=True)
                value = td.get_text(strip=True)
                
                if 'æ‰€åœ¨åœ°' in header:
                    address_text = re.sub(r'(åœ°å›³ã‚’è¦‹ã‚‹|ç„¡æ–™).*', '', value)
                    property_data['address'] = normalize_address(address_text.strip())  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
                elif 'åœŸåœ°é¢ç©' in header:
                    property_data['land_area'] = parse_area(value)  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
                elif 'å»ºç‰©é¢ç©' in header:
                    property_data['building_area'] = parse_area(value)  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
                elif 'ç¯‰å¹´æœˆ' in header:
                    property_data['construction_year'] = parse_construction_year(value)  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
                elif 'æ§‹é€ ' in header:
                    property_data['structure'] = value
    
    def _extract_contractor_info(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """å…ƒè«‹ä¼šç¤¾æƒ…å ±æŠ½å‡º"""
        contractor_info = {}
        
        # ä¼šç¤¾å
        company_elem = soup.select_one(self.selectors['company_name'])
        if company_elem:
            contractor_info['contractor_company_name'] = company_elem.get_text(strip=True)
        
        # é›»è©±ç•ªå·
        phone_elem = soup.select_one(self.selectors['company_phone'])
        if phone_elem:
            phone_text = phone_elem.get_text(strip=True)
            contractor_info['contractor_phone'] = clean_phone_number(phone_text)  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
        
        return contractor_info
    
    def _post_process(self, property_data: Dict[str, Any]):
        """å¾Œå‡¦ç†ï¼ˆå…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒªæ´»ç”¨ï¼‰"""
        if 'source_url' in property_data:
            property_data['listing_id'] = extract_listing_id(property_data['source_url'], "homes")  # å…±é€šãƒ©ã‚¤ãƒ–ãƒ©ãƒª
    
    def _find_next_page(self, soup: BeautifulSoup, current_url: str) -> Optional[str]:
        """æ¬¡ãƒšãƒ¼ã‚¸æ¤œç´¢"""
        for selector in self.selectors['next_page']:
            next_elem = soup.select_one(selector)
            if next_elem and next_elem.get('href'):
                return urljoin(current_url, next_elem['href'])
        return None
    
    def get_queue_stats(self) -> Dict[str, int]:
        """çµ±è¨ˆæƒ…å ±å–å¾—"""
        return self.url_queue.get_stats()
    
    def reset_queue(self):
        """ã‚­ãƒ¥ãƒ¼ãƒªã‚»ãƒƒãƒˆ"""
        self.url_queue.reset()
    
    # äº’æ›æ€§ãƒ¡ã‚½ãƒƒãƒ‰
    def scrape_listing_page(self, url: str) -> List[Dict[str, Any]]:
        self.collect_property_urls(url, max_pages=1)
        return []
    
    def scrape_detail_page(self, url: str) -> Optional[Dict[str, Any]]:
        return self.scrape_property_detail(url)


# ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
if __name__ == "__main__":
    logger.info("ğŸš€ [Homes] Test execution start")
    
    scraper = HomesPropertyScraper()
    
    try:
        # URLåé›†ãƒ†ã‚¹ãƒˆ
        test_url = "https://www.homes.co.jp/kodate/hokkaido/list/"
        collected = scraper.collect_property_urls(test_url, max_pages=2)
        logger.info(f"âœ… Collected {collected} URLs")
        
        # ãƒãƒƒãƒå‡¦ç†ãƒ†ã‚¹ãƒˆ
        properties = scraper.process_batch(batch_size=3)
        
        if properties:
            logger.info(f"âœ… Scraped {len(properties)} properties")
            for prop in properties[:2]:  # æœ€åˆã®2ä»¶ã ã‘è¡¨ç¤º
                logger.info(f"  - {prop.get('title')}")
                logger.info(f"    ğŸ’° {prop.get('price_raw')}")
                logger.info(f"    ğŸ¢ {prop.get('contractor_company_name', 'N/A')}")
        
        # çµ±è¨ˆ
        stats = scraper.get_queue_stats()
        logger.info(f"ğŸ“Š Stats: {stats}")
        
    except Exception as e:
        logger.error(f"âŒ Test failed: {e}")
    finally:
        scraper.close()
        logger.info("ğŸ Test completed")